# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cRgnkKJuYDkfFsTCgF1i__NfiznyvhX2
"""

import streamlit as st
import nltk
import ssl
import requests
import feedparser
from bs4 import BeautifulSoup
from docx import Document
from docx.shared import Pt
from docx.enum.text import WD_ALIGN_PARAGRAPH
from datetime import datetime
import time
import random
import os
import io
import urllib.parse
import re

# Configuración de página Streamlit
st.set_page_config(
    page_title="Noticias de Cuba",
    page_icon="📰",
    layout="wide",
    initial_sidebar_state="expanded"
)
def separar_palabras(texto):
    """
    Separa y limpia el texto, corrigiendo espacios y puntuación
    """
    # Reemplazar múltiples espacios con un solo espacio
    texto = ' '.join(texto.split())

    # Asegurar espacios después de puntuación
    for punct in ['.', ',', ';', ':', '!', '?']:
        texto = texto.replace(punct, punct + ' ')

    # Corregir espacios múltiples
    texto = ' '.join(texto.split())

    # Corregir espacios antes de puntuación
    for punct in [' .', ' ,', ' ;', ' :', ' !', ' ?']:
        texto = texto.replace(punct, punct.strip())

    # Separar palabras con mayúsculas en medio
    palabras = []
    for palabra in texto.split():
        if any(c.isupper() for c in palabra[1:]):
            nueva_palabra = ''
            for i, c in enumerate(palabra):
                if i > 0 and c.isupper():
                    nueva_palabra += ' ' + c
                else:
                    nueva_palabra += c
            palabras.extend(nueva_palabra.split())
        else:
            palabras.append(palabra)

    return ' '.join(palabras)

def limpiar_resumen(texto, titulo):
    """
    Limpia el texto del resumen eliminando elementos no deseados
    """
    try:
        texto = BeautifulSoup(texto, 'lxml').get_text()
        texto = separar_palabras(texto)

        # Eliminar URLs
        texto = re.sub(r'http\S+', '', texto)

        # Eliminar textos no deseados
        textos_no_deseados = [
            'Lea también', 'Lea más', 'Relacionado', 'Compartir',
            'facebook', 'twitter', 'RELACIONADO:', 'Etiquetas:',
            'Tags:', 'Autor:', titulo, 'The post',
            'first appeared on', 'appeared first on',
            'Noticias Prensa Latina', 'Noticias.',
            'Cubadebate', 'OnCuba'
        ]

        for texto_no_deseado in textos_no_deseados:
            texto = texto.replace(texto_no_deseado, '')

        # Limpieza final
        texto = ' '.join(texto.split()).strip()
        if texto and not texto.endswith('.'):
            texto += '.'

        return texto

    except Exception as e:
        st.error(f"Error limpiando resumen: {str(e)}")
        return texto

def generar_resumen_mejorado(texto, min_palabras=80, max_palabras=100):
    """
    Genera un resumen del texto entre min_palabras y max_palabras
    """
    try:
        # Limpieza inicial
        texto = texto.replace('\n', ' ')
        texto = limpiar_resumen(texto, '')

        palabras_total = len(texto.split())
        if palabras_total <= max_palabras:
            return texto

        oraciones = nltk.sent_tokenize(texto)
        oraciones_importantes = []
        palabras_acumuladas = 0

        # Incluir primera oración siempre
        primera_oracion = oraciones[0]
        oraciones_importantes.append(primera_oracion)
        palabras_acumuladas += len(primera_oracion.split())

        # Procesar resto de oraciones
        for oracion in oraciones[1:]:
            palabras_oracion = len(oracion.split())

            if palabras_acumuladas < min_palabras:
                oraciones_importantes.append(oracion)
                palabras_acumuladas += palabras_oracion
                continue

            if min_palabras <= palabras_acumuladas <= max_palabras:
                if palabras_acumuladas + palabras_oracion <= max_palabras + 10:
                    oraciones_importantes.append(oracion)
                    palabras_acumuladas += palabras_oracion
                else:
                    break

        resumen_texto = ' '.join(oraciones_importantes)
        resumen_texto = separar_palabras(resumen_texto)

        if not resumen_texto.endswith(('.', '!', '?', '...')):
            resumen_texto += '...'

        return resumen_texto

    except Exception as e:
        st.error(f"Error en generación de resumen: {str(e)}")
        return texto[:500] + '...'

def crear_documento_word(noticias, output):
    """
    Crea un documento Word con las noticias recopiladas
    """
    try:
        doc = Document()

        # Configurar estilo
        style = doc.styles['Normal']
        style.font.name = 'Arial'
        style.font.size = Pt(11)

        # Título principal
        titulo = doc.add_heading('Resumen de Noticias de Cuba', 0)
        titulo.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Fecha actual
        fecha = doc.add_paragraph()
        fecha.add_run(datetime.now().strftime("%d de %B de %Y")).bold = True
        fecha.alignment = WD_ALIGN_PARAGRAPH.CENTER

        doc.add_paragraph()

        # Organizar noticias por fuente
        fuentes = {
            'OnCuba News': [],
            'Cubadebate': [],
            'Prensa Latina': []
        }

        for noticia in noticias:
            if noticia['fuente'] in fuentes:
                fuentes[noticia['fuente']].append(noticia)

        # Agregar noticias por cada fuente
        for fuente, noticias_fuente in fuentes.items():
            if noticias_fuente:
                doc.add_heading(f'Noticias de {fuente}', 1)

                for i, noticia in enumerate(noticias_fuente, 1):
                    # Título de la noticia
                    doc.add_heading(f"{i}. {noticia['titulo']}", level=2)

                    # Metadatos
                    meta = doc.add_paragraph()
                    meta.add_run(f"Fecha: {noticia['fecha']}\n").italic = True
                    meta.add_run(f"Fuente: {noticia['url']}").italic = True

                    # Resumen
                    doc.add_paragraph()
                    p = doc.add_paragraph()
                    p.add_run(noticia['resumen'])

                    # Separador
                    doc.add_paragraph('_' * 50)

                doc.add_paragraph()

        # Guardar documento
        if isinstance(output, str):
            doc.save(output)
        else:
            doc.save(output)  # output es un BytesIO object

    except Exception as e:
        st.error(f"Error creando documento Word: {str(e)}")
        raise e

# Aplicar estilo CSS personalizado
st.markdown("""
    <style>
    .main {
        padding: 2rem;
    }
    .stButton>button {
        width: 100%;
        height: 3rem;
        margin: 1rem 0;
    }
    .noticia {
        padding: 1rem;
        border: 1px solid #ddd;
        border-radius: 5px;
        margin: 1rem 0;
    }
    .fuente-titulo {
        color: #1f77b4;
        font-size: 1.5rem;
        margin: 2rem 0 1rem 0;
    }
    .noticia-titulo {
        color: #2c3e50;
        font-size: 1.2rem;
        margin-bottom: 0.5rem;
    }
    .noticia-meta {
        color: #7f8c8d;
        font-size: 0.9rem;
        margin-bottom: 0.5rem;
    }
    .noticia-resumen {
        color: #34495e;
        font-size: 1rem;
        line-height: 1.5;
    }
    </style>
    """, unsafe_allow_html=True)

# Inicialización de NLTK
@st.cache_resource
def inicializar_nltk():
    try:
        _create_unverified_https_context = ssl._create_unverified_context
    except AttributeError:
        pass
    else:
        ssl._create_default_https_context = _create_unverified_https_context

    nltk.download('punkt')
    nltk.download('punkt_tab')
    nltk.download('averaged_perceptron_tagger')

# Función para mostrar el progreso
def mostrar_progreso(texto):
    with st.empty():
        st.write(texto)

# [Aquí van todas las funciones de procesamiento que ya teníamos]
# separar_palabras(), limpiar_resumen(), generar_resumen_mejorado(), etc.

def obtener_contenido_completo(url, headers=None):
    if headers is None:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8'
        }

    try:
        response = requests.get(url, headers=headers, timeout=30)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'lxml')

            content_selectors = [
                '.jeg_post_content',
                '.content-inner',
                '.entry-content',
                '.post-content',
                'article .content',
                '.article-content',
                '.content-area'
            ]

            for selector in content_selectors:
                content = soup.select_one(selector)
                if content:
                    paragraphs = content.find_all('p')
                    if paragraphs:
                        texto = ' '.join(p.get_text(strip=True) for p in paragraphs)
                        if len(texto) > 100:
                            return texto

        return ""
    except Exception as e:
        st.error(f"Error obteniendo contenido completo: {str(e)}")
        return ""

def obtener_noticias_oncuba():
    st.write("Obteniendo noticias de OnCuba News...")

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8'
    }

    urls = [
        'https://oncubanews.com/cuba/',
        'https://oncubanews.com/noticias/'
    ]

    noticias = []
    urls_procesadas = set()

    for url in urls:
        try:
            st.write(f"Conectando a: {url}")
            response = requests.get(url, headers=headers, timeout=15)

            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'lxml')
                articles = soup.select('div.jeg_posts article') or soup.select('article.jeg_post')

                if articles:
                    for article in articles[:10]:
                        try:
                            titulo_elem = article.select_one('h3.jeg_post_title a')
                            if not titulo_elem:
                                continue

                            titulo = titulo_elem.get_text(strip=True)
                            link = titulo_elem['href']

                            if link in urls_procesadas:
                                continue

                            urls_procesadas.add(link)

                            fecha_elem = article.select_one('.jeg_meta_date a')
                            fecha = fecha_elem.get_text(strip=True) if fecha_elem else datetime.now().strftime('%d/%m/%Y')

                            st.write(f"Procesando: {titulo}")

                            contenido = obtener_contenido_completo(link, headers)

                            if not contenido:
                                excerpt = article.select_one('.jeg_post_excerpt p')
                                if excerpt:
                                    contenido = excerpt.get_text(strip=True)

                            if contenido and len(contenido.split()) >= 80:
                                resumen = generar_resumen_mejorado(contenido)

                                noticias.append({
                                    'titulo': titulo,
                                    'texto': contenido,
                                    'resumen': resumen,
                                    'url': link,
                                    'fecha': fecha,
                                    'fuente': 'OnCuba News'
                                })

                                st.write("✓ Noticia procesada exitosamente")

                            if len(noticias) >= 5:
                                return noticias

                            time.sleep(random.uniform(1, 2))

                        except Exception as e:
                            st.error(f"Error procesando artículo: {str(e)}")
                            continue

        except Exception as e:
            st.error(f"Error con {url}: {str(e)}")
            continue

    return noticias

def obtener_noticias_cubadebate():
    try:
        st.write("Obteniendo noticias de Cubadebate...")

        url = "http://www.cubadebate.cu/feed"
        feed = feedparser.parse(url)

        if not feed.entries:
            st.warning("No se encontraron noticias en Cubadebate")
            return []

        noticias = []
        for entrada in feed.entries[:5]:
            try:
                titulo = entrada.title.strip()
                url_noticia = entrada.link

                st.write(f"Procesando: {titulo}")

                contenido = obtener_contenido_completo(url_noticia)
                if not contenido and hasattr(entrada, 'summary'):
                    contenido = entrada.summary

                contenido = limpiar_resumen(contenido, titulo)
                resumen = generar_resumen_mejorado(contenido)

                if hasattr(entrada, 'published_parsed'):
                    fecha = time.strftime('%d/%m/%Y %H:%M', entrada.published_parsed)
                else:
                    fecha = datetime.now().strftime('%d/%m/%Y %H:%M')

                noticias.append({
                    'titulo': titulo,
                    'texto': contenido,
                    'resumen': resumen,
                    'url': url_noticia,
                    'fecha': fecha,
                    'fuente': 'Cubadebate'
                })

                st.write("✓ Noticia procesada exitosamente")

                time.sleep(2)

            except Exception as e:
                st.error(f"Error procesando noticia: {str(e)}")
                continue

        return noticias

    except Exception as e:
        st.error(f"Error obteniendo noticias de Cubadebate: {str(e)}")
        return []

def obtener_noticias_prensa_latina():
    try:
        st.write("Obteniendo noticias de Prensa Latina...")

        url = "https://www.prensa-latina.cu/feed"
        feed = feedparser.parse(url)

        if not feed.entries:
            st.warning("No se encontraron noticias en Prensa Latina")
            return []

        noticias = []
        noticias_procesadas = 0

        for entrada in feed.entries:
            try:
                titulo = entrada.title
                if any(palabra in titulo.lower() for palabra in [
                    'lista', 'principales temas', 'minuto a minuto',
                    'suscripción', 'suscribirse'
                ]):
                    continue

                titulo = titulo.replace(" - Noticias Prensa Latina", "")
                titulo = titulo.replace(" | Prensa Latina", "")

                url_noticia = entrada.link

                st.write(f"Procesando: {titulo}")

                contenido = obtener_contenido_completo(url_noticia)
                if not contenido and hasattr(entrada, 'summary'):
                    contenido = entrada.summary

                contenido = limpiar_resumen(contenido, titulo)
                resumen = generar_resumen_mejorado(contenido)

                if len(resumen.split()) < 30:
                    continue

                if hasattr(entrada, 'published_parsed'):
                    fecha = time.strftime('%d/%m/%Y %H:%M', entrada.published_parsed)
                else:
                    fecha = datetime.now().strftime('%d/%m/%Y %H:%M')

                noticias.append({
                    'titulo': titulo,
                    'texto': contenido,
                    'resumen': resumen,
                    'url': url_noticia,
                    'fecha': fecha,
                    'fuente': 'Prensa Latina'
                })

                st.write("✓ Noticia procesada exitosamente")

                noticias_procesadas += 1
                if noticias_procesadas >= 5:
                    break

                time.sleep(2)

            except Exception as e:
                st.error(f"Error procesando noticia: {str(e)}")
                continue

        return noticias

    except Exception as e:
        st.error(f"Error obteniendo noticias de Prensa Latina: {str(e)}")
        return []



# Modificamos la función obtener_todas_las_noticias para mostrar progreso
def obtener_todas_las_noticias():
    todas_las_noticias = []

    fuentes = [
        ('OnCuba News', obtener_noticias_oncuba),
        ('Cubadebate', obtener_noticias_cubadebate),
        ('Prensa Latina', obtener_noticias_prensa_latina)
    ]

    progress_bar = st.progress(0)
    status_text = st.empty()

    for i, (nombre_fuente, funcion) in enumerate(fuentes):
        try:
            status_text.text(f"Obteniendo noticias de {nombre_fuente}...")
            noticias_fuente = funcion()

            if noticias_fuente:
                todas_las_noticias.extend(noticias_fuente)
                st.success(f"✓ Se obtuvieron {len(noticias_fuente)} noticias de {nombre_fuente}")
            else:
                st.warning(f"No se pudieron obtener noticias de {nombre_fuente}")

            progress_bar.progress((i + 1) / len(fuentes))
            time.sleep(1)

        except Exception as e:
            st.error(f"Error obteniendo noticias de {nombre_fuente}: {str(e)}")
            continue

    progress_bar.empty()
    status_text.empty()

    return todas_las_noticias

# Interfaz principal de Streamlit
def main():
    st.title("📰 Recopilador de Noticias de Cuba")
    st.markdown("""
    Esta aplicación recopila las últimas noticias de:
    - OnCuba News
    - Cubadebate
    - Prensa Latina
    """)

    # Sidebar
    with st.sidebar:
        st.header("Opciones")
        auto_download = st.checkbox("Descargar documento automáticamente", value=True)
        mostrar_detalles = st.checkbox("Mostrar detalles del proceso", value=False)

    # Botón principal
    if st.button("🔄 Obtener Noticias"):
        try:
            with st.spinner('Obteniendo noticias...'):
                noticias = obtener_todas_las_noticias()

                if noticias:
                    # Crear documento Word
                    nombre_archivo = f"Noticias_Cuba_{datetime.now().strftime('%Y%m%d_%H%M')}.docx"

                    # Usar BytesIO para manejar el archivo en memoria
                    docx_buffer = io.BytesIO()
                    crear_documento_word(noticias, docx_buffer)
                    docx_buffer.seek(0)

                    # Botón de descarga
                    st.download_button(
                        label="📥 Descargar Documento Word",
                        data=docx_buffer,
                        file_name=nombre_archivo,
                        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                    )

                    # Mostrar resumen
                    st.success(f"Se obtuvieron {len(noticias)} noticias en total")

                    # Mostrar noticias en la web
                    for fuente in ['OnCuba News', 'Cubadebate', 'Prensa Latina']:
                        noticias_fuente = [n for n in noticias if n['fuente'] == fuente]
                        if noticias_fuente:
                            st.markdown(f"### 📑 Noticias de {fuente}")
                            for noticia in noticias_fuente:
                                with st.expander(noticia['titulo']):
                                    st.markdown(f"**Fecha:** {noticia['fecha']}")
                                    st.markdown(f"**URL:** [{noticia['url']}]({noticia['url']})")
                                    st.markdown("**Resumen:**")
                                    st.write(noticia['resumen'])

                else:
                    st.error("No se pudieron obtener noticias")

        except Exception as e:
            st.error(f"Error en el proceso: {str(e)}")

if __name__ == "__main__":
    inicializar_nltk()
    main()



